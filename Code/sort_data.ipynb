{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_diff(df, census):\n",
    "\n",
    "    grp = df.groupby([\"plot\",  census])\n",
    "    cen = grp.date.agg(['min', 'max'])\n",
    "    cen[\"mid\"] = (cen[\"min\"] + (cen[\"max\"] - cen[\"min\"])/2).dt.date\n",
    "    cen[\"difference\"] = cen[\"mid\"].diff().astype('timedelta64[D]')\n",
    "    cen.loc[cen[\"difference\"] < 0 , \"difference\"] = np.NAN\n",
    "    cen[\"diff_yrs\"] = cen.difference/365\n",
    "\n",
    "    cen.reset_index(level=0, inplace=True)\n",
    "    cen.reset_index(level=0, inplace=True)\n",
    "\n",
    "    cen[census].astype(str)\n",
    "    cen[\"step\"] = cen[census].astype(str).shift() + \"-\" + cen[census].astype(str)\n",
    "    cen.loc[cen[\"difference\"].isnull(), \"step\"] = np.NaN\n",
    "    cen.index = cen['plot'] + \"_\" + cen['step']\n",
    "\n",
    "    return cen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open general data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open plot locations\n",
    "with open('../Data/rows.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "properties = pd.DataFrame()\n",
    "\n",
    "# add each line of geojson file to dataframe\n",
    "for i in range(0, len(data['features'])):\n",
    "    properties = properties.append(pd.DataFrame(data['features'][i]['properties'], index = [i]))\n",
    "\n",
    "# only really care about these columns\n",
    "properties = properties.loc[: , [\"plot_size\",\n",
    "                                 \"centroid_y\",\n",
    "                                 \"centroid_x\",\n",
    "                                 \"fractal_order\",\n",
    "                                 \"location\"]]\n",
    "\n",
    "properties.rename(columns={'centroid_y':'longitude', 'centroid_x':'latitude'}, inplace=True)\n",
    "properties[\"longlat\"] = properties.apply(lambda x: [x.longitude, x.latitude], axis=1)\n",
    "\n",
    "# seperate dataframe for only second order fractal points\n",
    "second_order = properties.loc[properties.fractal_order == 2, : ]\n",
    "\n",
    "# function to find point closest to given point\n",
    "def closest(pt, others):\n",
    "    \n",
    "    clst_pt = min(others.longlat, key = lambda x: geodesic(pt, x).meters)\n",
    "    distnce = geodesic(pt, clst_pt).meters\n",
    "    clst_pt = others.location.loc[others.longlat.apply(lambda x: x == clst_pt)].reset_index(drop = True)\n",
    "\n",
    "    return pd.Series([clst_pt.values[0], distnce])\n",
    "\n",
    "\n",
    "# fractal nesting and agb data\n",
    "fpn = pd.read_csv(\"../Data/Fractal_point_nesting.csv\")\n",
    "agb = pd.read_csv(\"../Data/AGB.csv\")\n",
    "\n",
    "fpn[\"FirstOrder\"] = fpn.FirstOrder.str.partition(\"_\")[2].astype(int)\n",
    "fpn.columns = [\"site\",\n",
    "               \"habitat\",\n",
    "               \"logging\",\n",
    "               \"frag_area\",\n",
    "               \"first_order\",\n",
    "               \"second_order\",\n",
    "               \"third_order\",\n",
    "               \"fourth_order\",\n",
    "               \"fifth_order\"]\n",
    "fractals = fpn.loc[:, [\"first_order\", \"second_order\"]]\n",
    "\n",
    "# specific wanted columns - and rename ***(going with Chave moist)***\n",
    "agb = agb[[\"field_name\", \"Plot\", \"Date\", \"AGB_Chave_moist\", \"ForestQuality\"]]\n",
    "agb.columns = [\"field_name\", \"second_order\", \"date\", \"agb\", \"forestquality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mammals Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open and sort the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open each seperate plot\n",
    "E  = pd.read_csv(\"../Data/small_mammals/test/E_test.csv\")\n",
    "F  = pd.read_csv(\"../Data/small_mammals/test/F_test.csv\")\n",
    "D  = pd.read_csv(\"../Data/small_mammals/test/D_test.csv\")\n",
    "#OG = pd.read_csv(\"../Data/small_mammals/test/OG_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to sort out each plot in turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_mams(df):\n",
    "\n",
    "    # new column names\n",
    "    ncolnames   = [\"occasion\", \"date\", \"grid\", \"point\", \"trap\", \"trap_id\", \"species\"]\n",
    "    df.columns  = ncolnames\n",
    "\n",
    "    # stupid formatting sorted\n",
    "    df[\"occasion\"]  = df.occasion.str.replace(\"--\", \"-\")\n",
    "    df[\"plot\"]      = df.occasion.str[0]\n",
    "    #df[\"plot\"]      = df.grid  **think it would be worth giving this another go!**\n",
    "    df[\"grid\"]      = df.grid.str.replace(\"--\", \"-\")\n",
    "    df[\"trap_id\"]   = df.trap_id.str.replace(\"--\", \"-\")\n",
    "    df[\"trap_id\"]   = df.trap_id.apply(lambda x: x[:-1])\n",
    "    df[\"date\"]      = pd.to_datetime(df.date)\n",
    "    df[\"year\"]      = df.date.dt.year\n",
    "    df[\"census\"]    = df.occasion.str.partition(\"-\")[2].str.partition(\"-\")[2]\n",
    "    \n",
    "    return df\n",
    "\n",
    "E  = sort_mams(E)\n",
    "F  = sort_mams(F)\n",
    "D  = sort_mams(D)\n",
    "#OG = sort_mams(OG)\n",
    "\n",
    "frames = [E, F, D]\n",
    "\n",
    "mamls_df = pd.concat(frames, sort = False)\n",
    "\n",
    "mamls_df[\"species\"] = mamls_df.species.fillna(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### match with species names from lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mammals species lookup table \n",
    "m_lkup = pd.read_csv(\"../Data/small_mammals/mammals_lookup.csv\")\n",
    "m_lkup.columns = [\"code\", \"species\", \"scientific\"]\n",
    "\n",
    "mamls_df[\"species\"] = mamls_df.species.str.strip()\n",
    "\n",
    "\n",
    "# my fairly questionable decisions...\n",
    "\n",
    "# if its a questionmark - I just go with it\n",
    "# if its an either or I go with the first one!\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"CTRS-but see notes\", \"species\"] = \"CTRS\"\n",
    "\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"SS?\",          \"species\"] = \"SS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"WH?\",          \"species\"] = \"WH\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"PR?\",          \"species\"] = \"PR\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"RR?\",          \"species\"] = \"RR\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"MR?\",          \"species\"] = \"MR\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"MR??\",         \"species\"] = \"MR\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"RS?\",          \"species\"] = \"RS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"LGTRS?\",       \"species\"] = \"LGTRS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"BS?\",          \"species\"] = \"BS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"PSQ\",          \"species\"] = \"LSQ\"      # not confident on this\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"BSQ\",          \"species\"] = \"BSQ?\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"SSQ\",          \"species\"] = \"SSQ?\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"RS or SS\" ,    \"species\"] = \"RS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"WH or SS\",     \"species\"] = \"WH\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"BS/RS?\",       \"species\"] = \"BS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"PTSQ?\",        \"species\"] = \"PTSQ\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"LETRS\",        \"species\"] = \"LETRS?\"   # for some reason the lookup table has a ?\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"CBS?\",         \"species\"] = \"CBS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"SL?TRS\",       \"species\"] = \"SLTRS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"SLTRS?\",       \"species\"] = \"SLTRS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"L?TRS\",        \"species\"] = \"SLTRS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"LSQ?\",         \"species\"] = \"LSQ\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"CTRS?\",        \"species\"] = \"CTRS\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"LTRS or CTRS\", \"species\"] = \"CTRS\"     # went with CTRS as LTRS could refer to a couple\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"LTRS\",         \"species\"] = \"LETRS?\"   # not convinced about this one\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"Squirrel\",     \"species\"] = \"squirrel\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"DTT_DEAD\",     \"species\"] = \"DTT\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"LSQ?_DEAD\",    \"species\"] = \"LSQ\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"squirrel\",     \"species\"] = \"unknown\"  # ***mmm?***\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"See notes\",    \"species\"] = \"unknown\"  # i'm effectivley treating 'unknown' as a seperate species which seems spurious at best \n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"??\",           \"species\"] = \"unknown\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"?\",            \"species\"] = \"unknown\"\n",
    "mamls_df.loc[mamls_df.loc[:, \"species\"] == \"Unknown\",      \"species\"] = \"unknown\"\n",
    "\n",
    "# merge\n",
    "mamls_df = pd.merge(mamls_df,\n",
    "                    m_lkup[[\"code\", \"scientific\"]],\n",
    "                    how      = \"left\",\n",
    "                    left_on  = \"species\",\n",
    "                    right_on = \"code\")\n",
    "\n",
    "# get rid of the leftovers... (there were a couple of birds/reptiles)\n",
    "mamls_df = mamls_df.loc[-mamls_df.code.isna(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find closest f2 point to each trap and get agb measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique trap names\n",
    "trap_locs = pd.DataFrame({\"trap_id\" : mamls_df.trap_id.unique()})\n",
    "\n",
    "trap_locs = trap_locs.merge(properties[[\"location\", \"longlat\"]],\n",
    "                            how      = \"left\",\n",
    "                            left_on  = \"trap_id\",\n",
    "                            right_on = \"location\")\n",
    "\n",
    "# find the closest second order fractal point\n",
    "trap_locs[[\"second_order\", \"distance_so\"]] = trap_locs.longlat.apply(lambda x: closest(x, second_order))\n",
    "\n",
    "# merge back to master dataframe\n",
    "mamls_df = mamls_df.merge(trap_locs, how = \"left\", on = \"trap_id\")\n",
    "\n",
    "# just want the point number \n",
    "mamls_df.second_order = mamls_df.second_order.str[-3:]\n",
    "mamls_df.second_order = mamls_df.second_order.astype(int)\n",
    "\n",
    "# merge to get agb and forest quality\n",
    "mamls_df = mamls_df.merge(agb[[\"second_order\", \"agb\", \"forestquality\"]], how = \"left\",\n",
    "                          on = \"second_order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final cleanup and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mamls_df = mamls_df.rename(index=str, columns={\"plot_x\": \"plot\"})\n",
    "\n",
    "mamls_df = mamls_df[[\"occasion\",\n",
    "                     \"date\",\n",
    "                     \"grid\",\n",
    "                     \"point\",\n",
    "                     \"trap\",\n",
    "                     \"trap_id\",\n",
    "                     \"species\",\n",
    "                     \"year\",\n",
    "                     \"plot\",\n",
    "                     \"census\",\n",
    "                     \"scientific\",\n",
    "                     \"longlat\",\n",
    "                     \"second_order\",\n",
    "                     \"distance_so\",\n",
    "                     \"agb\",\n",
    "                     \"forestquality\"]]\n",
    "\n",
    "mamls_df.to_csv(\"../Results/mammals_sorted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the species/plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i'll give four different combinations a go...\n",
    "mamls_df[\"trap_year\"]   = mamls_df[\"plot\"] + \"_\" + mamls_df.trap_id + \"_\" + mamls_df.year.astype(str)\n",
    "mamls_df[\"grid_year\"]   = mamls_df[\"plot\"] + \"_\" + mamls_df.grid    + \"_\" + mamls_df.year.astype(str)\n",
    "mamls_df[\"trap_census\"] = mamls_df[\"plot\"] + \"_\" + mamls_df.trap_id + \"_\" + mamls_df.census\n",
    "mamls_df[\"grid_census\"] = mamls_df[\"plot\"] + \"_\" + mamls_df.grid    + \"_\" + mamls_df.census\n",
    "\n",
    "# function to make matrix\n",
    "def make_matrix(df, what):\n",
    "    mx = df.groupby([what, \"species\"]).size().unstack()  # groupby whatever i've chosen and species\n",
    "    mx = mx.fillna(value = 0)                            # fill with 0's\n",
    "    mx = mx.drop(\"None\", axis = 1)                       # so we keep plot row even if nothing was trapped\n",
    "    return mx\n",
    "\n",
    "# do\n",
    "mamls_TY = make_matrix(mamls_df, \"trap_year\")\n",
    "mamls_GY = make_matrix(mamls_df, \"grid_year\")\n",
    "mamls_TC = make_matrix(mamls_df, \"trap_census\")\n",
    "mamls_GC = make_matrix(mamls_df, \"grid_census\")\n",
    "\n",
    "# save\n",
    "mamls_TY.to_csv(\"../Results/m_trap-year.csv\")\n",
    "mamls_GY.to_csv(\"../Results/m_grid-year.csv\")\n",
    "mamls_TC.to_csv(\"../Results/m_trap-census.csv\")\n",
    "mamls_GC.to_csv(\"../Results/m_grid_census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the agb (either mean or median of all the traps in plot)\n",
    "mamls_agb = mamls_df.groupby(\"plot\").agb.describe()\n",
    "mamls_agb = pd.DataFrame(mamls_agb[\"50%\"])\n",
    "mamls_agb.to_csv(\"../Results/mamls_agb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardise time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and time difference between cesuses - i'm doing year or occasion...\n",
    "mamls_cn_diff = census_diff(mamls_df, \"census\")\n",
    "mamls_yr_diff = census_diff(mamls_df, \"year\")\n",
    "\n",
    "mamls_yr_diff = mamls_yr_diff.rename(index=str, columns={\"year\": \"census\"})\n",
    "\n",
    "mamls_cn_diff.to_csv(\"../Results/mamls_census_dates.csv\")\n",
    "mamls_yr_diff.to_csv(\"../Results/mamls_years_dates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### readin raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readin RAW data\n",
    "trees_df = pd.read_csv(\"../Data/SAFE_CarbonPlots_Tree+LianaCensus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to sort everything out for each census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(df, census_no):  # give new column names, delete NAs and dead...\n",
    "\n",
    "    # consistant and better column names\n",
    "    new_Cnames = ['f_type',       # forest type\n",
    "                  'plot',\n",
    "                  'subplot',\n",
    "                  'date',         # date of measurements\n",
    "                  'observers',\n",
    "                  'tag_no',\n",
    "                  'd_pom',        # diameter of tree (cm)\n",
    "                  'h_pom',        # height diameter is taken (m) 1.3 by default\n",
    "                  'height',\n",
    "                  'flag',         # condition of trees (see flag list)\n",
    "                  'alive',        # 1 = yes, NaN = no\n",
    "                  'stem_C',       # aboveground biomass of tree (kg)\n",
    "                  'root_C',       # root biomass of tree\n",
    "                  'field_cmnts',  # comments from field\n",
    "                  'data_cmnts',   # comments from data entry\n",
    "                  'sbplt_X',\n",
    "                  'sbplt_Y',\n",
    "                  'CPA',          # projected area of the crown of the stem\n",
    "                  'X_FMC',        # plot level X coordinate\n",
    "                  'Y_FMC',        # plot level Y coordinte\n",
    "                  'Z_FMC',        # plot level elevation\n",
    "                  'family',\n",
    "                  'binomial',\n",
    "                  'wood_density']\n",
    "\n",
    "    # give each census these column names\n",
    "    df.columns = new_Cnames\n",
    "\n",
    "    # get unique ID - combine plot and tag_no\n",
    "    df = df.assign(ID = df['plot'] + df['tag_no'].map(str))\n",
    "\n",
    "    # column with census number\n",
    "    df = df.assign(census = census_no)\n",
    "\n",
    "    # delete rows with NaNs in important columns\n",
    "    impt_cols = ['tag_no', 'd_pom', 'h_pom', 'height', 'flag', 'alive',\n",
    "                 'stem_C', 'root_C']\n",
    "\n",
    "    df = df.dropna(subset = impt_cols, how = 'all')\n",
    "\n",
    "    # delete dead trees (alive == 0)\n",
    "    df = df[df.alive == 1]\n",
    "\n",
    "    # sort out dates\n",
    "    df.date = pd.to_datetime(df.date, dayfirst = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subset each census - weird column things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for each census\n",
    "census_1 = trees_df.iloc[ :, list(range(0, 3))     # same for all\n",
    "                           + list(range(3, 15))    # specific for census\n",
    "                           + list(range(53, 62))]  # same for all\n",
    "\n",
    "census_2 = trees_df.iloc[ :, list(range(0, 3))\n",
    "                           + list(range(15, 27))\n",
    "                           + list(range(53, 62))]\n",
    "\n",
    "census_3 = trees_df.iloc[ :, list(range(0, 3))\n",
    "                           + list(range(27, 39))\n",
    "                           + list(range(53, 62))]\n",
    "\n",
    "census_4 = trees_df.iloc[ :, list(range(0, 3))\n",
    "                           + list(range(39, 51))\n",
    "                           + list(range(53, 62))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do function and combine census'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data for each census\n",
    "census_1 = sort_data(census_1, 1)\n",
    "census_2 = sort_data(census_2, 2)\n",
    "census_3 = sort_data(census_3, 3)\n",
    "census_4 = sort_data(census_4, 4)\n",
    "\n",
    "# recombine all census data (stack on top of each other)\n",
    "trees_df = pd.concat([census_1, census_2, census_3, census_4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add extra columns and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add genus column\n",
    "trees_df['plot']        = trees_df['plot'].replace(\" \", \"\", regex=True)\n",
    "trees_df['subplot']     = trees_df['subplot'].apply(lambda x: str(x).zfill(2))\n",
    "trees_df['genus']       = trees_df.apply(lambda row: row.binomial.split(\" \")[0], axis = 1)\n",
    "trees_df['plt_sub']     = trees_df['plot'] + \"_sp\" + trees_df['subplot'].astype(str)\n",
    "trees_df['plt_sub_cen'] = trees_df['plt_sub'] + \"_c\" + trees_df['census'].astype(str)\n",
    "trees_df['plot_c']      = trees_df['plot'] + \"_c\" + trees_df['census'].astype(str)\n",
    "trees_df['census']      = \"c\" + trees_df.census.astype(str)\n",
    "\n",
    "# save to csv\n",
    "trees_df.to_csv(\"../Results/trees_sorted.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species matrix\n",
    "trees_matrix = trees_df.groupby(['plt_sub_cen', 'binomial']).size().unstack()\n",
    "trees_matrix = trees_matrix.fillna(value = 0)\n",
    "trees_matrix.to_csv(\"../Results/trees_matrix.csv\")\n",
    "\n",
    "trees_genus_matrix = trees_df.groupby(['plt_sub_cen', 'genus']).size().unstack()\n",
    "trees_genus_matrix = trees_genus_matrix.fillna(value = 0)\n",
    "trees_genus_matrix.to_csv(\"../Results/trees_genus_matrix.csv\")\n",
    "\n",
    "trees_family_matrix = trees_df.groupby(['plt_sub_cen', 'family']).size().unstack()\n",
    "trees_family_matrix = trees_family_matrix.fillna(value = 0)\n",
    "trees_family_matrix.to_csv(\"../Results/trees_family_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trees agb (different from the others as calculated from data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total biomass at each census for each plot\n",
    "trees_agb = pd.DataFrame(trees_df.groupby([\"plot\", \"census\"]).stem_C.sum())\n",
    "\n",
    "# take the mean of all census' 0.0625 and 0.001 to get it into Mg/0.0625 ha\n",
    "trees_agb = pd.DataFrame(trees_agb.groupby(\"plot\").mean()*  0.0625 * 0.001)\n",
    "\n",
    "trees_agb.to_csv(\"../Results/trees_agb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trees census standardise time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_cen = census_diff(trees_df, \"census\")\n",
    "trees_cen.to_csv(\"../Results/trees_census_dates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beetles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "btles_df = pd.read_csv(\"../Data/family_list.csv\", index_col = 0)\n",
    "\n",
    "# convert dates to datetime\n",
    "btles_df.date = pd.to_datetime(btles_df.date, dayfirst = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "btles_df = btles_df.merge(fractals, how = \"left\", left_on = \"trap_N\", right_on = \"first_order\")\n",
    "btles_df[\"second_order\"] = btles_df[\"second_order\"].str.partition(\"_\")[2].astype(int)\n",
    "btles_df = btles_df.merge(agb[[\"second_order\", \"agb\", \"forestquality\"]], how = \"left\",\n",
    "                          on = \"second_order\")\n",
    "btles_df = btles_df.rename(index=str, columns={\"block\": \"plot\"})\n",
    "\n",
    "btles_df[\"subplot\"] = btles_df[\"plot\"] + \"-\" + btles_df.first_order.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorting out census'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling periods\n",
    "btles_df['census'] = \"incomplete\"\n",
    "\n",
    "\n",
    "def sample_period(df, s_date, e_date, period):\n",
    "    df.loc[(df.date >= s_date) & (df.date < e_date), 'census'] = period\n",
    "\n",
    "s1 = pd.to_datetime(\"01/01/2011\", dayfirst = True)  # might be good to check!!\n",
    "e1 = pd.to_datetime(\"01/04/2011\", dayfirst = True)\n",
    "s2 = pd.to_datetime(\"01/09/2011\", dayfirst = True)\n",
    "e2 = pd.to_datetime(\"01/01/2012\", dayfirst = True)\n",
    "s3 = pd.to_datetime(\"01/04/2012\", dayfirst = True)\n",
    "e3 = pd.to_datetime(\"01/09/2012\", dayfirst = True)\n",
    "\n",
    "sample_period(btles_df, s1, e1, \"P1\")\n",
    "sample_period(btles_df, s2, e2, \"P2\")\n",
    "sample_period(btles_df, s3, e3, \"P3\")\n",
    "\n",
    "btles_df = btles_df[btles_df.census != \"incomplete\"]\n",
    "\n",
    "btles_df['plt_sub_cen'] = btles_df[\"plot\"] + \"_\" + btles_df.subplot + \"_\" + btles_df.census\n",
    "\n",
    "btles_df.to_csv(\"../Results/btles_sorted.csv\", index = False)\n",
    "\n",
    "\n",
    "btles_mx = btles_df.groupby(['plt_sub_cen', 'family']).size().unstack()\n",
    "btles_mx = btles_mx.fillna(value = 0)\n",
    "btles_mx.to_csv(\"../Results/btles_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time between censuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "btles_cen = census_diff(btles_df, \"census\")\n",
    "btles_cen.to_csv(\"../Results/btles_census_dates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "btles_agb = pd.DataFrame(btles_df.groupby(\"plot\").agb.median())\n",
    "\n",
    "btles_agb.to_csv(\"../Results/btles_agb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mozzies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie1 = pd.read_csv(\"../Data/DailyHLC2012-2013.csv\")\n",
    "mozie2 = pd.read_csv(\"../Data/DailyHLC2013-2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie1[\"census\"] = \"c1\"\n",
    "mozie2[\"census\"] = \"c2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [mozie1, mozie2]\n",
    "mozie  = pd.concat(frames, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie = mozie.drop(['field_name',\n",
    "                    'Collector',\n",
    "                    'Moonlight',\n",
    "                    'Forest_cover',\n",
    "                    'Height',\n",
    "                    'Tree_Height',\n",
    "                    'Wind',\n",
    "                    'Rain',\n",
    "                    'Temperature',\n",
    "                    'Humidity'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie_meta  = mozie[['Date', 'Location', 'Disturbance', 'census']]\n",
    "mozie_count = mozie.drop(['Date', 'Location', 'Disturbance', 'census'], axis = 1)\n",
    "\n",
    "mozie_meta.columns = [\"date\", \"location\", \"disturbance\", \"census\"]\n",
    "\n",
    "mozie_count = mozie_count.fillna(0)\n",
    "mozie_count.columns = pd.Series(mozie_count.columns).str.rpartition(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [mozie_meta, mozie_count]\n",
    "mozie = pd.concat([mozie_meta, mozie_count], axis = 1,sort = False)\n",
    "mozie = mozie.reset_index()\n",
    "\n",
    "mozie = mozie.drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie[\"date\"] = pd.to_datetime(mozie.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of Da_Tree\n",
    "mozie = mozie.loc[mozie.location.str.rpartition(\"_\")[0] != \"Da_Tree\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie[[\"plot\", \"second_order\"]] = mozie.location.str.partition(\"_\")[[0,2]]\n",
    "\n",
    "mozie.second_order = mozie.second_order.astype(int)\n",
    "\n",
    "mozie = mozie.merge(agb[[\"second_order\", \"agb\", \"forestquality\"]], how = \"left\", on = \"second_order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie['plt_sub_cen'] = mozie[\"plot\"] + \"_\" + mozie.second_order.astype(str) + \"_\" + mozie.census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie.to_csv(\"../Results/mozie_sorted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie.index = mozie.plt_sub_cen\n",
    "\n",
    "mozie_mx = mozie.drop([\"date\",\n",
    "                       \"location\",\n",
    "                       \"disturbance\",\n",
    "                       \"census\",\n",
    "                       \"agb\",\n",
    "                       \"forestquality\",\n",
    "                       \"plot\",\n",
    "                       \"second_order\",\n",
    "                       \"plt_sub_cen\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie_cen = census_diff(mozie, \"census\")\n",
    "mozie_cen.to_csv(\"../Results/mozie_census_dates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozie_agb = pd.DataFrame(mozie.groupby(\"plot\").agb.median())\n",
    "\n",
    "mozie_agb.to_csv(\"../Results/btles_agb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plot</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>1.217018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>4.140394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG2</th>\n",
       "      <td>29.274437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG3</th>\n",
       "      <td>18.551408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP1</th>\n",
       "      <td>0.186782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP2</th>\n",
       "      <td>0.321447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP3</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VJR</th>\n",
       "      <td>15.262123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agb\n",
       "plot           \n",
       "D      1.217018\n",
       "E      4.140394\n",
       "OG2   29.274437\n",
       "OG3   18.551408\n",
       "OP1    0.186782\n",
       "OP2    0.321447\n",
       "OP3    0.000000\n",
       "VJR   15.262123"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mozie_agb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
